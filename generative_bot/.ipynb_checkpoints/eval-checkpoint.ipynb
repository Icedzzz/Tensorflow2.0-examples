{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JIT Compiled ChatSpace Model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from model.seq2seq.Seq2Seq import *\n",
    "from model.seq2seq_attn.Seq2Seq_Attn import *\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine what kind of model type we will use\n",
    "# e.g. seq2seq, seq2seq_attn\n",
    "model_type = 'seq2seq'\n",
    "attn_type = 'luong'\n",
    "method = 'dot'\n",
    "\n",
    "# GPU:1 allocation\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    \n",
    "    # Load data\n",
    "    dataset = load_dataset(data_dir)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    enc_tokenizer = load_tokenizer('enc-tokenizer')\n",
    "    dec_tokenizer = load_tokenizer('dec-tokenizer')\n",
    "    enc_vocab_size = enc_tokenizer.vocab_size + 1\n",
    "    dec_vocab_size = dec_tokenizer.vocab_size + 2    \n",
    "    \n",
    "    # Define the optimizer and the loss function\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    \n",
    "    if model_type == 'seq2seq':\n",
    "        # Set a configuration\n",
    "        config = {'batch_size': 1,\n",
    "                  'enc_max_len': enc_max_len+1,\n",
    "                  'dec_max_len': dec_max_len+2,\n",
    "                  'enc_unit': enc_unit,\n",
    "                  'dec_unit': dec_unit,\n",
    "                  'embed_dim': embed_dim,\n",
    "                  'dropout_rate': dropout_rate,\n",
    "                  'enc_vocab_size': enc_vocab_size,\n",
    "                  'dec_vocab_size': dec_vocab_size,\n",
    "                  'dec_sos_token': dec_tokenizer.vocab_size\n",
    "                  }\n",
    "\n",
    "        # Define the seq2seq model\n",
    "        model = seq2seq(config)\n",
    "\n",
    "        # Set a checkpoint directory\n",
    "        checkpoint_dir = 'checkpoint/daily-korean/seq2seq'\n",
    "        \n",
    "    elif model_type == 'seq2seq_attn':\n",
    "        # Set a configuration\n",
    "        config = {'batch_size': 1,\n",
    "                  'enc_max_len': enc_max_len+1,\n",
    "                  'dec_max_len': dec_max_len+2,\n",
    "                  'enc_unit': enc_unit,\n",
    "                  'dec_unit': dec_unit,\n",
    "                  'embed_dim': embed_dim,\n",
    "                  'dropout_rate': dropout_rate,\n",
    "                  'enc_vocab_size': enc_vocab_size,\n",
    "                  'dec_vocab_size': dec_vocab_size,\n",
    "                  'dec_sos_token': dec_tokenizer.vocab_size,\n",
    "                  'attn_type': attn_type,\n",
    "                  'method': method\n",
    "                  }\n",
    "\n",
    "        # Define the seq2seq model\n",
    "        model = seq2seq_attn(config)\n",
    "\n",
    "        # Set a checkpoint directory\n",
    "        checkpoint_dir = 'checkpoint/daily-korean/seq2seq_{}_attn'.format(attn_type)\n",
    "     \n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        input_text = input(\">: \")\n",
    "        \n",
    "        if input_text == 'q':\n",
    "            break\n",
    "        \n",
    "        enc_input = tf.keras.preprocessing.sequence.pad_sequences([enc_encode(input_text, enc_tokenizer)], \n",
    "                                                             maxlen=enc_max_len+1, padding='post')\n",
    "        \n",
    "        model.load_weights(filepath=tf.train.latest_checkpoint(checkpoint_dir))\n",
    "        \n",
    "        enc_tokens = idx2word(enc_input[0], enc_tokenizer, tokenizer_type='encoder')\n",
    "\n",
    "        outputs = model(enc_input, training=False)\n",
    "\n",
    "        if model_type == 'seq2seq':\n",
    "            preds = outputs\n",
    "        elif model_type == 'seq2seq_attn':\n",
    "            preds = outputs[0]\n",
    "            attn_weights = outputs[1]\n",
    "\n",
    "        pred_str, pred_tokens = decoding_from_result(preds, dec_tokenizer)\n",
    "        print(\"<: \", pred_str)\n",
    "        \n",
    "        if model_type == 'seq2seq_attn':\n",
    "            # plotting the attention weights\n",
    "            plotly_attention(attn_weights, enc_tokens, pred_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">:  안녕하세요\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 26)\n",
      "<:  좋은 일이 될 거예요 .\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'attn_weights' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-194f89fa89f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-247bc4e4b068>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# plotting the attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mplotly_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'attn_weights' referenced before assignment"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
