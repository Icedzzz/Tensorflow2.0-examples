{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from model.seq2seq.Seq2Seq import *\n",
    "from model.seq2seq_attn.Seq2Seq_Attn import *\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine what kind of model type we will use\n",
    "# e.g. seq2seq, seq2seq_attn\n",
    "model_type = 'seq2seq_attn'\n",
    "attn_type = 'luong'\n",
    "method = 'dot'\n",
    "\n",
    "# GPU:1 allocation\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def loss_function(true, pred, loss_obj):\n",
    "    mask = tf.math.logical_not(tf.math.equal(true, 0))\n",
    "\n",
    "    loss = loss_obj(true, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Load data\n",
    "    dataset = load_dataset(data_dir)\n",
    "    \n",
    "    num_batches_per_epoch = len(dataset) // batch_size\n",
    "    \n",
    "    # Load tokenizer\n",
    "    enc_tokenizer = load_tokenizer('enc-tokenizer', (x for x, y in dataset), target_vocab_size=2**13)\n",
    "    dec_tokenizer = load_tokenizer('dec-tokenizer', (y for x, y in dataset), target_vocab_size=2**13)\n",
    "    enc_vocab_size = enc_tokenizer.vocab_size + 1\n",
    "    dec_vocab_size = dec_tokenizer.vocab_size + 2\n",
    "    print(f'enc_vocab_size: {enc_vocab_size}\\tdec_vocab_size: {dec_vocab_size}')\n",
    "    \n",
    "    # Define the optimizer and the loss function\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    \n",
    "    if model_type == 'seq2seq':\n",
    "        # Define the seq2seq model\n",
    "        model = Seq2Seq(batch_size=batch_size, \n",
    "                        enc_max_len=enc_max_len+1, \n",
    "                        dec_max_len=dec_max_len+2, \n",
    "                        enc_unit=enc_unit, \n",
    "                        dec_unit=dec_unit, \n",
    "                        embed_dim=embed_dim, \n",
    "                        dropout_rate=dropout_rate,\n",
    "                        enc_vocab_size=enc_vocab_size, \n",
    "                        dec_vocab_size=dec_vocab_size, \n",
    "                        dec_sos_token=dec_tokenizer.vocab_size)\n",
    "\n",
    "        # Set a checkpoint directory\n",
    "        checkpoint_dir = 'checkpoint/daily-korean/seq2seq'\n",
    "        \n",
    "    elif model_type == 'seq2seq_attn':\n",
    "        # Define the seq2seq model\n",
    "        model = AttnSeq2Seq(batch_size=batch_size, \n",
    "                            enc_max_len=enc_max_len+1, \n",
    "                            dec_max_len=dec_max_len+2, \n",
    "                            enc_unit=enc_unit, \n",
    "                            dec_unit=dec_unit, \n",
    "                            embed_dim=embed_dim, \n",
    "                            dropout_rate=dropout_rate,\n",
    "                            enc_vocab_size=enc_vocab_size, \n",
    "                            dec_vocab_size=dec_vocab_size, \n",
    "                            dec_sos_token=dec_tokenizer.vocab_size,\n",
    "                            attn_type=attn_type,\n",
    "                            method=method)\n",
    "        \n",
    "        # Set a checkpoint directory\n",
    "        checkpoint_dir = 'checkpoint/daily-korean/seq2seq_{}_attn'.format(attn_type)\n",
    "        \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "        \n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, 'checkpoint')\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "    \n",
    "    epoch_loss = tf.keras.metrics.Mean()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss.reset_states()\n",
    "        \n",
    "        train_batches = batch_dataset(dataset, batch_size, enc_tokenizer, dec_tokenizer, enc_max_len, dec_max_len)\n",
    "        \n",
    "        for batch_idx, (batch_x, batch_y) in enumerate(train_batches):\n",
    "            loss = 0.\n",
    "            with tf.GradientTape() as tape:\n",
    "                outputs = model(batch_x, batch_y, True)\n",
    "                \n",
    "                if model_type == 'seq2seq':\n",
    "                    preds = outputs\n",
    "                elif model_type == 'seq2seq_attn':\n",
    "                    preds = outputs[0]\n",
    "                    attn_weights = outputs[1]\n",
    "                    \n",
    "                loss = loss_function(batch_y[:, 1:], preds, loss_obj)\n",
    "            \n",
    "            variables = model.trainable_variables\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(gradients, variables))\n",
    "            \n",
    "            epoch_loss(loss)\n",
    "            \n",
    "            if (batch_idx + 1) % log_interval == 0:\n",
    "                print(f'[Epoch {epoch + 1}|Step {batch_idx + 1}/{num_batches_per_epoch}] loss: {loss.numpy()} (Avg. {epoch_loss.result()})')\n",
    "        \n",
    "        model.save_weights(filepath=checkpoint_prefix)\n",
    "    \n",
    "    print(\"Training is Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(1234)\n",
    "    tf.random.set_seed(1234)\n",
    "    \n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
