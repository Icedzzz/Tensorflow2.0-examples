{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JIT Compiled ChatSpace Model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from model.seq2seq.Seq2Seq import *\n",
    "from model.seq2seq_attn.Seq2Seq_Attn import *\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def loss_function(true, pred, loss_obj):\n",
    "    mask = tf.math.logical_not(tf.math.equal(true, 0))\n",
    "\n",
    "    loss = loss_obj(true, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Load data\n",
    "    dataset = load_dataset(data_dir)\n",
    "    \n",
    "    num_batches_per_epoch = len(dataset) // batch_size\n",
    "    \n",
    "    # Load tokenizer\n",
    "    enc_tokenizer = load_tokenizer('enc-tokenizer', (x for x, y in dataset), target_vocab_size=2**13)\n",
    "    dec_tokenizer = load_tokenizer('dec-tokenizer', (y for x, y in dataset), target_vocab_size=2**13)\n",
    "    enc_vocab_size = enc_tokenizer.vocab_size + 1\n",
    "    dec_vocab_size = dec_tokenizer.vocab_size + 2\n",
    "    print(f'enc_vocab_size: {enc_vocab_size}\\tdec_vocab_size: {dec_vocab_size}')\n",
    "    \n",
    "    # Define the optimizer and the loss function\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    \n",
    "    # Define seq2seq model\n",
    "    config = {'batch_size': batch_size,\n",
    "              'enc_max_len': enc_max_len+1,\n",
    "              'dec_max_len': dec_max_len+2,\n",
    "              'enc_unit': enc_unit,\n",
    "              'dec_unit': dec_unit,\n",
    "              'embed_dim': embed_dim,\n",
    "              'dropout_rate': dropout_rate,\n",
    "              'enc_vocab_size': enc_vocab_size,\n",
    "              'dec_vocab_size': dec_vocab_size,\n",
    "              'dec_sos_token': dec_tokenizer.vocab_size\n",
    "              }\n",
    "    \n",
    "    model = seq2seq(config)\n",
    "    \n",
    "    # checkpoint\n",
    "    checkpoint_dir = 'checkpoint/daily-korean/seq2seq'\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "        \n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, 'checkpoint')\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "    \n",
    "    epoch_loss = tf.keras.metrics.Mean()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss.reset_states()\n",
    "        \n",
    "        train_batches = batch_dataset(dataset, batch_size, enc_tokenizer, dec_tokenizer, enc_max_len, dec_max_len)\n",
    "        \n",
    "        for batch_idx, (batch_x, batch_y) in enumerate(train_batches):\n",
    "            loss = 0.\n",
    "            with tf.GradientTape() as tape:\n",
    "                preds = model(batch_x, batch_y, True)\n",
    "\n",
    "                loss = loss_function(batch_y[:, 1:], preds, loss_obj)\n",
    "            \n",
    "            variables = model.trainable_variables\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(gradients, variables))\n",
    "            \n",
    "            epoch_loss(loss)\n",
    "            \n",
    "            if (batch_idx + 1) % log_interval == 0:\n",
    "                print(f'[Epoch {epoch + 1}|Step {batch_idx + 1}/{num_batches_per_epoch}] loss: {loss.numpy()} (Avg. {epoch_loss.result()})')\n",
    "        \n",
    "        model.save_weights(filepath=checkpoint_prefix)\n",
    "    \n",
    "    print(\"Training is Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_vocab_size: 8633\tdec_vocab_size: 7921\n",
      "11823\n",
      "[Epoch 1|Step 50/369] loss: 1.5011221170425415 (Avg. 1.7559351921081543)\n",
      "[Epoch 1|Step 100/369] loss: 1.5256730318069458 (Avg. 1.6276884078979492)\n",
      "[Epoch 1|Step 150/369] loss: 1.4489439725875854 (Avg. 1.5577911138534546)\n",
      "[Epoch 1|Step 200/369] loss: 1.3613492250442505 (Avg. 1.5276087522506714)\n",
      "[Epoch 1|Step 250/369] loss: 1.333289623260498 (Avg. 1.4995362758636475)\n",
      "[Epoch 1|Step 300/369] loss: 1.2491003274917603 (Avg. 1.47150719165802)\n",
      "[Epoch 1|Step 350/369] loss: 1.1868115663528442 (Avg. 1.4474748373031616)\n",
      "11823\n",
      "[Epoch 2|Step 50/369] loss: 1.1757022142410278 (Avg. 1.2392525672912598)\n",
      "[Epoch 2|Step 100/369] loss: 1.2452367544174194 (Avg. 1.2425786256790161)\n",
      "[Epoch 2|Step 150/369] loss: 1.2470276355743408 (Avg. 1.2250256538391113)\n",
      "[Epoch 2|Step 200/369] loss: 1.0841407775878906 (Avg. 1.2170518636703491)\n",
      "[Epoch 2|Step 250/369] loss: 1.213249683380127 (Avg. 1.2139114141464233)\n",
      "[Epoch 2|Step 300/369] loss: 1.016128659248352 (Avg. 1.2131340503692627)\n",
      "[Epoch 2|Step 350/369] loss: 1.062329649925232 (Avg. 1.2094309329986572)\n",
      "11823\n",
      "[Epoch 3|Step 50/369] loss: 1.1078736782073975 (Avg. 1.0874824523925781)\n",
      "[Epoch 3|Step 100/369] loss: 1.077109694480896 (Avg. 1.0967814922332764)\n",
      "[Epoch 3|Step 150/369] loss: 1.0662480592727661 (Avg. 1.0850359201431274)\n",
      "[Epoch 3|Step 200/369] loss: 1.194625973701477 (Avg. 1.0876879692077637)\n",
      "[Epoch 3|Step 250/369] loss: 1.1625362634658813 (Avg. 1.088906168937683)\n",
      "[Epoch 3|Step 300/369] loss: 1.0681512355804443 (Avg. 1.0863442420959473)\n",
      "[Epoch 3|Step 350/369] loss: 1.1222399473190308 (Avg. 1.0874590873718262)\n",
      "11823\n",
      "[Epoch 4|Step 50/369] loss: 1.001313328742981 (Avg. 0.9472971558570862)\n",
      "[Epoch 4|Step 100/369] loss: 0.8369023203849792 (Avg. 0.9443440437316895)\n",
      "[Epoch 4|Step 150/369] loss: 0.876311719417572 (Avg. 0.9585257172584534)\n",
      "[Epoch 4|Step 200/369] loss: 0.903384268283844 (Avg. 0.9640583992004395)\n",
      "[Epoch 4|Step 250/369] loss: 1.04917573928833 (Avg. 0.9629454612731934)\n",
      "[Epoch 4|Step 300/369] loss: 0.7812042832374573 (Avg. 0.963492214679718)\n",
      "[Epoch 4|Step 350/369] loss: 0.7206601500511169 (Avg. 0.9599813222885132)\n",
      "11823\n",
      "[Epoch 5|Step 50/369] loss: 0.8303210735321045 (Avg. 0.8029902577400208)\n",
      "[Epoch 5|Step 100/369] loss: 0.9550737142562866 (Avg. 0.8195321559906006)\n",
      "[Epoch 5|Step 150/369] loss: 0.7836886644363403 (Avg. 0.8243803977966309)\n",
      "[Epoch 5|Step 200/369] loss: 0.8194926977157593 (Avg. 0.8200778365135193)\n",
      "[Epoch 5|Step 250/369] loss: 0.9366415739059448 (Avg. 0.8259654641151428)\n",
      "[Epoch 5|Step 300/369] loss: 0.7294571399688721 (Avg. 0.8303463459014893)\n",
      "[Epoch 5|Step 350/369] loss: 0.875612199306488 (Avg. 0.8306673765182495)\n",
      "11823\n",
      "[Epoch 6|Step 50/369] loss: 0.6167977452278137 (Avg. 0.6910424828529358)\n",
      "[Epoch 6|Step 100/369] loss: 0.8228180408477783 (Avg. 0.688088059425354)\n",
      "[Epoch 6|Step 150/369] loss: 0.7221459150314331 (Avg. 0.6956037282943726)\n",
      "[Epoch 6|Step 200/369] loss: 0.6421010494232178 (Avg. 0.6978428363800049)\n",
      "[Epoch 6|Step 250/369] loss: 0.9197791814804077 (Avg. 0.6998608112335205)\n",
      "[Epoch 6|Step 300/369] loss: 0.7510190606117249 (Avg. 0.7036994099617004)\n",
      "[Epoch 6|Step 350/369] loss: 0.7471896409988403 (Avg. 0.7070950269699097)\n",
      "11823\n",
      "[Epoch 7|Step 50/369] loss: 0.626998782157898 (Avg. 0.5689016580581665)\n",
      "[Epoch 7|Step 100/369] loss: 0.6894881725311279 (Avg. 0.5785804986953735)\n",
      "[Epoch 7|Step 150/369] loss: 0.6729837656021118 (Avg. 0.5832951068878174)\n",
      "[Epoch 7|Step 200/369] loss: 0.6479116678237915 (Avg. 0.5891480445861816)\n",
      "[Epoch 7|Step 250/369] loss: 0.6293347477912903 (Avg. 0.5950222015380859)\n",
      "[Epoch 7|Step 300/369] loss: 0.5942716002464294 (Avg. 0.5978891253471375)\n",
      "[Epoch 7|Step 350/369] loss: 0.5924057364463806 (Avg. 0.6009621620178223)\n",
      "11823\n",
      "[Epoch 8|Step 50/369] loss: 0.4807402789592743 (Avg. 0.4733893573284149)\n",
      "[Epoch 8|Step 100/369] loss: 0.4954940676689148 (Avg. 0.4814733862876892)\n",
      "[Epoch 8|Step 150/369] loss: 0.5541470050811768 (Avg. 0.48810094594955444)\n",
      "[Epoch 8|Step 200/369] loss: 0.5500692129135132 (Avg. 0.49353542923927307)\n",
      "[Epoch 8|Step 250/369] loss: 0.5636564493179321 (Avg. 0.4938541352748871)\n",
      "[Epoch 8|Step 300/369] loss: 0.6394777297973633 (Avg. 0.5009656548500061)\n",
      "[Epoch 8|Step 350/369] loss: 0.6385408639907837 (Avg. 0.5052506327629089)\n",
      "11823\n",
      "[Epoch 9|Step 50/369] loss: 0.451305091381073 (Avg. 0.4071179926395416)\n",
      "[Epoch 9|Step 100/369] loss: 0.4327532649040222 (Avg. 0.4063333570957184)\n",
      "[Epoch 9|Step 150/369] loss: 0.4059760272502899 (Avg. 0.4145992398262024)\n",
      "[Epoch 9|Step 200/369] loss: 0.4136996567249298 (Avg. 0.41418740153312683)\n",
      "[Epoch 9|Step 250/369] loss: 0.42953217029571533 (Avg. 0.415883332490921)\n",
      "[Epoch 9|Step 300/369] loss: 0.4647563397884369 (Avg. 0.41899898648262024)\n",
      "[Epoch 9|Step 350/369] loss: 0.3962101936340332 (Avg. 0.4219776391983032)\n",
      "11823\n",
      "[Epoch 10|Step 50/369] loss: 0.35902735590934753 (Avg. 0.32438480854034424)\n",
      "[Epoch 10|Step 100/369] loss: 0.35025259852409363 (Avg. 0.3306201696395874)\n",
      "[Epoch 10|Step 150/369] loss: 0.3573330342769623 (Avg. 0.3319455087184906)\n",
      "[Epoch 10|Step 200/369] loss: 0.32001590728759766 (Avg. 0.3356955647468567)\n",
      "[Epoch 10|Step 250/369] loss: 0.41071462631225586 (Avg. 0.33987078070640564)\n",
      "[Epoch 10|Step 300/369] loss: 0.3393537402153015 (Avg. 0.34544244408607483)\n",
      "[Epoch 10|Step 350/369] loss: 0.36044612526893616 (Avg. 0.3498046398162842)\n",
      "11823\n",
      "[Epoch 11|Step 50/369] loss: 0.2358483374118805 (Avg. 0.2643146216869354)\n",
      "[Epoch 11|Step 100/369] loss: 0.2918979227542877 (Avg. 0.27056634426116943)\n",
      "[Epoch 11|Step 150/369] loss: 0.23911505937576294 (Avg. 0.27883341908454895)\n",
      "[Epoch 11|Step 200/369] loss: 0.35242220759391785 (Avg. 0.28247591853141785)\n",
      "[Epoch 11|Step 250/369] loss: 0.2598216235637665 (Avg. 0.284807413816452)\n",
      "[Epoch 11|Step 300/369] loss: 0.2859632074832916 (Avg. 0.2885711193084717)\n",
      "[Epoch 11|Step 350/369] loss: 0.26985403895378113 (Avg. 0.2903227210044861)\n",
      "11823\n",
      "[Epoch 12|Step 50/369] loss: 0.22460952401161194 (Avg. 0.22337192296981812)\n",
      "[Epoch 12|Step 100/369] loss: 0.23434661328792572 (Avg. 0.2283579260110855)\n",
      "[Epoch 12|Step 150/369] loss: 0.23269802331924438 (Avg. 0.22748827934265137)\n",
      "[Epoch 12|Step 200/369] loss: 0.24745041131973267 (Avg. 0.2318156659603119)\n",
      "[Epoch 12|Step 250/369] loss: 0.2162388116121292 (Avg. 0.23526279628276825)\n",
      "[Epoch 12|Step 300/369] loss: 0.2702476680278778 (Avg. 0.23794713616371155)\n",
      "[Epoch 12|Step 350/369] loss: 0.2115393429994583 (Avg. 0.23904038965702057)\n",
      "11823\n",
      "[Epoch 13|Step 50/369] loss: 0.25544244050979614 (Avg. 0.17706412076950073)\n",
      "[Epoch 13|Step 100/369] loss: 0.18529897928237915 (Avg. 0.18124598264694214)\n",
      "[Epoch 13|Step 150/369] loss: 0.1562093198299408 (Avg. 0.1823519766330719)\n",
      "[Epoch 13|Step 200/369] loss: 0.2439052164554596 (Avg. 0.18703269958496094)\n",
      "[Epoch 13|Step 250/369] loss: 0.17438003420829773 (Avg. 0.1881313920021057)\n",
      "[Epoch 13|Step 300/369] loss: 0.17675615847110748 (Avg. 0.190970778465271)\n",
      "[Epoch 13|Step 350/369] loss: 0.1942526400089264 (Avg. 0.19394001364707947)\n",
      "11823\n",
      "[Epoch 14|Step 50/369] loss: 0.16840584576129913 (Avg. 0.1445792317390442)\n",
      "[Epoch 14|Step 100/369] loss: 0.17656120657920837 (Avg. 0.14836977422237396)\n",
      "[Epoch 14|Step 150/369] loss: 0.17143544554710388 (Avg. 0.15027113258838654)\n",
      "[Epoch 14|Step 200/369] loss: 0.1504564881324768 (Avg. 0.1525527834892273)\n",
      "[Epoch 14|Step 250/369] loss: 0.1592322736978531 (Avg. 0.15346184372901917)\n",
      "[Epoch 14|Step 300/369] loss: 0.20110797882080078 (Avg. 0.15597911179065704)\n",
      "[Epoch 14|Step 350/369] loss: 0.22354036569595337 (Avg. 0.15791073441505432)\n",
      "11823\n",
      "[Epoch 15|Step 50/369] loss: 0.13442324101924896 (Avg. 0.10736548155546188)\n",
      "[Epoch 15|Step 100/369] loss: 0.11694646626710892 (Avg. 0.11248058080673218)\n",
      "[Epoch 15|Step 150/369] loss: 0.13962648808956146 (Avg. 0.11708300560712814)\n",
      "[Epoch 15|Step 200/369] loss: 0.09105049073696136 (Avg. 0.11909624934196472)\n",
      "[Epoch 15|Step 250/369] loss: 0.14165295660495758 (Avg. 0.12136823683977127)\n",
      "[Epoch 15|Step 300/369] loss: 0.15580040216445923 (Avg. 0.12444836646318436)\n",
      "[Epoch 15|Step 350/369] loss: 0.12442105263471603 (Avg. 0.12631787359714508)\n",
      "11823\n",
      "[Epoch 16|Step 50/369] loss: 0.08227653801441193 (Avg. 0.09331586211919785)\n",
      "[Epoch 16|Step 100/369] loss: 0.08650607615709305 (Avg. 0.09472623467445374)\n",
      "[Epoch 16|Step 150/369] loss: 0.10346292704343796 (Avg. 0.09591824561357498)\n",
      "[Epoch 16|Step 200/369] loss: 0.08414721488952637 (Avg. 0.09696098417043686)\n",
      "[Epoch 16|Step 250/369] loss: 0.08366067707538605 (Avg. 0.09857950359582901)\n",
      "[Epoch 16|Step 300/369] loss: 0.15171372890472412 (Avg. 0.09983121603727341)\n",
      "[Epoch 16|Step 350/369] loss: 0.10326392948627472 (Avg. 0.10177948325872421)\n",
      "11823\n",
      "[Epoch 17|Step 50/369] loss: 0.0807558223605156 (Avg. 0.07224313169717789)\n",
      "[Epoch 17|Step 100/369] loss: 0.06669270247220993 (Avg. 0.07360919564962387)\n",
      "[Epoch 17|Step 150/369] loss: 0.08254634588956833 (Avg. 0.07492943108081818)\n",
      "[Epoch 17|Step 200/369] loss: 0.10096346586942673 (Avg. 0.07638133317232132)\n",
      "[Epoch 17|Step 250/369] loss: 0.062019236385822296 (Avg. 0.07706482708454132)\n",
      "[Epoch 17|Step 300/369] loss: 0.08134981244802475 (Avg. 0.07890137284994125)\n",
      "[Epoch 17|Step 350/369] loss: 0.09782889485359192 (Avg. 0.08047809451818466)\n",
      "11823\n",
      "[Epoch 18|Step 50/369] loss: 0.04680253937840462 (Avg. 0.05566839128732681)\n",
      "[Epoch 18|Step 100/369] loss: 0.04585140198469162 (Avg. 0.05711427703499794)\n",
      "[Epoch 18|Step 150/369] loss: 0.06541071832180023 (Avg. 0.059743136167526245)\n",
      "[Epoch 18|Step 200/369] loss: 0.09896096587181091 (Avg. 0.06116793677210808)\n",
      "[Epoch 18|Step 250/369] loss: 0.051938917487859726 (Avg. 0.062068916857242584)\n",
      "[Epoch 18|Step 300/369] loss: 0.0844556987285614 (Avg. 0.06310217082500458)\n",
      "[Epoch 18|Step 350/369] loss: 0.08053675293922424 (Avg. 0.06420639902353287)\n",
      "11823\n",
      "[Epoch 19|Step 50/369] loss: 0.06573005020618439 (Avg. 0.043108366429805756)\n",
      "[Epoch 19|Step 100/369] loss: 0.03486170619726181 (Avg. 0.04501566290855408)\n",
      "[Epoch 19|Step 150/369] loss: 0.048336319625377655 (Avg. 0.045901283621788025)\n",
      "[Epoch 19|Step 200/369] loss: 0.04841206595301628 (Avg. 0.047259651124477386)\n",
      "[Epoch 19|Step 250/369] loss: 0.03648669645190239 (Avg. 0.04891263693571091)\n",
      "[Epoch 19|Step 300/369] loss: 0.0601285919547081 (Avg. 0.05028660222887993)\n",
      "[Epoch 19|Step 350/369] loss: 0.04891480877995491 (Avg. 0.051106762140989304)\n",
      "11823\n",
      "[Epoch 20|Step 50/369] loss: 0.04587564244866371 (Avg. 0.03717579320073128)\n",
      "[Epoch 20|Step 100/369] loss: 0.03345710039138794 (Avg. 0.0377124659717083)\n",
      "[Epoch 20|Step 150/369] loss: 0.031121645122766495 (Avg. 0.03879864513874054)\n",
      "[Epoch 20|Step 200/369] loss: 0.0472443513572216 (Avg. 0.03943444415926933)\n",
      "[Epoch 20|Step 250/369] loss: 0.07871631532907486 (Avg. 0.04025040939450264)\n",
      "[Epoch 20|Step 300/369] loss: 0.053957752883434296 (Avg. 0.04081011936068535)\n",
      "[Epoch 20|Step 350/369] loss: 0.030508605763316154 (Avg. 0.041731901466846466)\n",
      "Training is Done.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(1234)\n",
    "    tf.random.set_seed(1234)\n",
    "    \n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
