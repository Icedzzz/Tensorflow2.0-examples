{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JIT Compiled ChatSpace Model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from model.seq2seq.Seq2Seq import *\n",
    "from model.seq2seq_attn.Seq2Seq_Attn import *\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine what kind of model type we will use\n",
    "# e.g. seq2seq, seq2seq_attn\n",
    "model_type = 'seq2seq_attn'\n",
    "attn_type = 'luong'\n",
    "method = 'dot'\n",
    "\n",
    "# GPU:1 allocation\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def loss_function(true, pred, loss_obj):\n",
    "    mask = tf.math.logical_not(tf.math.equal(true, 0))\n",
    "\n",
    "    loss = loss_obj(true, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Load data\n",
    "    dataset = load_dataset(data_dir)\n",
    "    \n",
    "    num_batches_per_epoch = len(dataset) // batch_size\n",
    "    \n",
    "    # Load tokenizer\n",
    "    enc_tokenizer = load_tokenizer('enc-tokenizer', (x for x, y in dataset), target_vocab_size=2**13)\n",
    "    dec_tokenizer = load_tokenizer('dec-tokenizer', (y for x, y in dataset), target_vocab_size=2**13)\n",
    "    enc_vocab_size = enc_tokenizer.vocab_size + 1\n",
    "    dec_vocab_size = dec_tokenizer.vocab_size + 2\n",
    "    print(f'enc_vocab_size: {enc_vocab_size}\\tdec_vocab_size: {dec_vocab_size}')\n",
    "    \n",
    "    # Define the optimizer and the loss function\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    \n",
    "    if model_type == 'seq2seq':\n",
    "        # Set a configuration\n",
    "        config = {'batch_size': batch_size,\n",
    "                  'enc_max_len': enc_max_len+1,\n",
    "                  'dec_max_len': dec_max_len+2,\n",
    "                  'enc_unit': enc_unit,\n",
    "                  'dec_unit': dec_unit,\n",
    "                  'embed_dim': embed_dim,\n",
    "                  'dropout_rate': dropout_rate,\n",
    "                  'enc_vocab_size': enc_vocab_size,\n",
    "                  'dec_vocab_size': dec_vocab_size,\n",
    "                  'dec_sos_token': dec_tokenizer.vocab_size\n",
    "                  }\n",
    "\n",
    "        # Define the seq2seq model\n",
    "        model = seq2seq(config)\n",
    "\n",
    "        # Set a checkpoint directory\n",
    "        checkpoint_dir = 'checkpoint/daily-korean/seq2seq'\n",
    "        \n",
    "    elif model_type == 'seq2seq_attn':\n",
    "        # Set a configuration\n",
    "        config = {'batch_size': batch_size,\n",
    "                  'enc_max_len': enc_max_len+1,\n",
    "                  'dec_max_len': dec_max_len+2,\n",
    "                  'enc_unit': enc_unit,\n",
    "                  'dec_unit': dec_unit,\n",
    "                  'embed_dim': embed_dim,\n",
    "                  'dropout_rate': dropout_rate,\n",
    "                  'enc_vocab_size': enc_vocab_size,\n",
    "                  'dec_vocab_size': dec_vocab_size,\n",
    "                  'dec_sos_token': dec_tokenizer.vocab_size,\n",
    "                  'attn_type': attn_type,\n",
    "                  'method': method\n",
    "                  }\n",
    "\n",
    "        # Define the seq2seq model\n",
    "        model = seq2seq_attn(config)\n",
    "\n",
    "        # Set a checkpoint directory\n",
    "        checkpoint_dir = 'checkpoint/daily-korean/seq2seq_{}_attn'.format(attn_type)\n",
    "        \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "        \n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, 'checkpoint')\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "    \n",
    "    epoch_loss = tf.keras.metrics.Mean()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss.reset_states()\n",
    "        \n",
    "        train_batches = batch_dataset(dataset, batch_size, enc_tokenizer, dec_tokenizer, enc_max_len, dec_max_len)\n",
    "        \n",
    "        for batch_idx, (batch_x, batch_y) in enumerate(train_batches):\n",
    "            loss = 0.\n",
    "            with tf.GradientTape() as tape:\n",
    "                outputs = model(batch_x, batch_y, True)\n",
    "                \n",
    "                if model_type == 'seq2seq':\n",
    "                    preds = outputs\n",
    "                elif model_type == 'seq2seq_attn':\n",
    "                    preds = outputs[0]\n",
    "                    attn_weights = outputs[1]\n",
    "                    \n",
    "                loss = loss_function(batch_y[:, 1:], preds, loss_obj)\n",
    "            \n",
    "            variables = model.trainable_variables\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(gradients, variables))\n",
    "            \n",
    "            epoch_loss(loss)\n",
    "            \n",
    "            if (batch_idx + 1) % log_interval == 0:\n",
    "                print(f'[Epoch {epoch + 1}|Step {batch_idx + 1}/{num_batches_per_epoch}] loss: {loss.numpy()} (Avg. {epoch_loss.result()})')\n",
    "        \n",
    "        model.save_weights(filepath=checkpoint_prefix)\n",
    "    \n",
    "    print(\"Training is Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_vocab_size: 8633\tdec_vocab_size: 7921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0226 03:24:55.764105 140005697693504 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1|Step 50/369] loss: 1.4856469631195068 (Avg. 1.7386714220046997)\n",
      "[Epoch 1|Step 100/369] loss: 1.5124869346618652 (Avg. 1.6123584508895874)\n",
      "[Epoch 1|Step 150/369] loss: 1.4398281574249268 (Avg. 1.5446010828018188)\n",
      "[Epoch 1|Step 200/369] loss: 1.3429992198944092 (Avg. 1.5131295919418335)\n",
      "[Epoch 1|Step 250/369] loss: 1.3298581838607788 (Avg. 1.4850895404815674)\n",
      "[Epoch 1|Step 300/369] loss: 1.2465062141418457 (Avg. 1.4575309753417969)\n",
      "[Epoch 1|Step 350/369] loss: 1.1726826429367065 (Avg. 1.4339226484298706)\n",
      "[Epoch 2|Step 50/369] loss: 1.149631381034851 (Avg. 1.2216838598251343)\n",
      "[Epoch 2|Step 100/369] loss: 1.2455790042877197 (Avg. 1.2257002592086792)\n",
      "[Epoch 2|Step 150/369] loss: 1.2269006967544556 (Avg. 1.2083889245986938)\n",
      "[Epoch 2|Step 200/369] loss: 1.0734096765518188 (Avg. 1.2010623216629028)\n",
      "[Epoch 2|Step 250/369] loss: 1.191190481185913 (Avg. 1.1984002590179443)\n",
      "[Epoch 2|Step 300/369] loss: 1.0066502094268799 (Avg. 1.1980574131011963)\n",
      "[Epoch 2|Step 350/369] loss: 1.0588394403457642 (Avg. 1.1948630809783936)\n",
      "[Epoch 3|Step 50/369] loss: 1.083641767501831 (Avg. 1.059777855873108)\n",
      "[Epoch 3|Step 100/369] loss: 1.0703380107879639 (Avg. 1.0683698654174805)\n",
      "[Epoch 3|Step 150/369] loss: 1.0511507987976074 (Avg. 1.0566569566726685)\n",
      "[Epoch 3|Step 200/369] loss: 1.1616429090499878 (Avg. 1.0587974786758423)\n",
      "[Epoch 3|Step 250/369] loss: 1.1438453197479248 (Avg. 1.0605396032333374)\n",
      "[Epoch 3|Step 300/369] loss: 1.0389602184295654 (Avg. 1.058433175086975)\n",
      "[Epoch 3|Step 350/369] loss: 1.1013871431350708 (Avg. 1.059624433517456)\n",
      "Training is Done.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(1234)\n",
    "    tf.random.set_seed(1234)\n",
    "    \n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
