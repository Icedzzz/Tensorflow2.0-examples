{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JIT Compiled ChatSpace Model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from model.seq2seq.Seq2Seq import *\n",
    "from model.seq2seq_attn.Seq2Seq_Attn import *\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    \n",
    "    # Load tokenizer\n",
    "    enc_tokenizer = load_tokenizer('enc-tokenizer')\n",
    "    dec_tokenizer = load_tokenizer('dec-tokenizer')\n",
    "    enc_vocab_size = enc_tokenizer.vocab_size + 1\n",
    "    dec_vocab_size = dec_tokenizer.vocab_size + 2\n",
    "    \n",
    "    # Define the optimizer and the loss function\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    \n",
    "    # Define seq2seq model\n",
    "    config = {'batch_size': 1,\n",
    "              'enc_max_len': enc_max_len+1,\n",
    "              'dec_max_len': dec_max_len+2,\n",
    "              'enc_unit': enc_unit,\n",
    "              'dec_unit': dec_unit,\n",
    "              'embed_dim': embed_dim,\n",
    "              'dropout_rate': dropout_rate,\n",
    "              'enc_vocab_size': enc_vocab_size,\n",
    "              'dec_vocab_size': dec_vocab_size,\n",
    "              'dec_sos_token': dec_tokenizer.vocab_size}\n",
    "\n",
    "    #model = seq2seq(config)\n",
    "    model = seq2seq_attn(config)\n",
    "    \n",
    "    # checkpoint\n",
    "    checkpoint_dir = 'checkpoint/daily-korean/seq2seq_attn'\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        input_text = input(\">: \")\n",
    "        \n",
    "        if input_text == 'q':\n",
    "            break\n",
    "        \n",
    "        enc_input = tf.keras.preprocessing.sequence.pad_sequences([enc_encode(input_text, enc_tokenizer)], \n",
    "                                                             maxlen=enc_max_len+1, padding='post')\n",
    "\n",
    "        model.load_weights(filepath=tf.train.latest_checkpoint(checkpoint_dir))\n",
    "        \n",
    "        enc_tokens = idx2word(enc_input[0], enc_tokenizer)\n",
    "        print(enc_tokens)\n",
    "        preds, attn_weights = model(enc_input, training=False)\n",
    "\n",
    "        pred_str, pred_tokens = decoding_from_result(preds, dec_tokenizer)\n",
    "        print(\"<: \", pred_str)\n",
    "        \n",
    "        print(attn_weights.shape, len(enc_tokens), len(pred_tokens))\n",
    "        print(attn_weights)\n",
    "        # plotting the attention weights\n",
    "        plot_attention(attn_weights, enc_tokens, pred_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">:  안녕하세요\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<:  안녕하세요 .\n",
      "(1, 26, 26) 2 2\n",
      "tf.Tensor(\n",
      "[[[0.00789661 0.01597753 0.97612584 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.09487355 0.12389449 0.781232   0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.2252377  0.21163902 0.5631233  0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.01750437 0.02704174 0.9554539  0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.28889206 0.327012   0.3840959  0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.21172361 0.25821385 0.5300625  0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.1644526  0.26156896 0.5739785  0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.23494108 0.337886   0.42717296 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.17152216 0.23593168 0.59254616 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.2289933  0.32911983 0.4418868  0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.18895443 0.2562934  0.55475223 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.22380532 0.31474277 0.46145198 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.19930224 0.2645127  0.5361851  0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.2212932  0.303      0.4757068  0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.20686944 0.27368045 0.5194501  0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.21892844 0.2942966  0.48677498 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.21115677 0.2781465  0.5106967  0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.21769445 0.28884232 0.49346322 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.21387681 0.28061095 0.50551224 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.21689285 0.28530827 0.4977989  0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.215375   0.28147066 0.50315434 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.21650067 0.28304085 0.5004585  0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.21609502 0.2813727  0.5025323  0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.21635072 0.2815181  0.5021312  0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.21635613 0.28075957 0.5028843  0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]\n",
      "  [0.21631768 0.2803955  0.50328684 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.        ]]], shape=(1, 26, 26), dtype=float32)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dec_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-194f89fa89f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-d0dfa597729a>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# plotting the attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mplot_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/notebooks/Project/chatbot/demo/utils/utils.py\u001b[0m in \u001b[0;36mplot_attention\u001b[0;34m(attention_weights, enc_tokens, pred_tokens)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdec_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0menc_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dec_len' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
