{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JIT Compiled ChatSpace Model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from model.seq2seq.Seq2Seq import *\n",
    "from model.seq2seq_attn.Seq2Seq_Attn import *\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_type = 'luong'\n",
    "method = 'dot'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(true, pred, loss_obj):\n",
    "    mask = tf.math.logical_not(tf.math.equal(true, 0))\n",
    "\n",
    "    loss = loss_obj(true, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    # Load data\n",
    "    dataset = load_dataset(data_dir)\n",
    "    #dataset = load_morp_dataset(dataset)\n",
    "    \n",
    "    #num_batches_per_epoch = len(dataset) // batch_size\n",
    "    \n",
    "    # Load tokenizer\n",
    "    enc_tokenizer = load_tokenizer('enc-tokenizer', (x for x, y in dataset), target_vocab_size=2**13)\n",
    "    dec_tokenizer = load_tokenizer('dec-tokenizer', (y for x, y in dataset), target_vocab_size=2**13)\n",
    "    enc_vocab_size = enc_tokenizer.vocab_size + 1\n",
    "    dec_vocab_size = dec_tokenizer.vocab_size + 2\n",
    "    print(f'enc_vocab_size: {enc_vocab_size}\\tdec_vocab_size: {dec_vocab_size}')\n",
    "    \n",
    "    #word2idx = build_dict(dataset)\n",
    "    #idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    #vocab_size = len(word2idx)\n",
    "    #print(\"Size of word2idx is {}\".format(len(word2idx)))\n",
    "    \n",
    "    # Define the optimizer and the loss function\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    \n",
    "    # Define seq2seq model\n",
    "    config = {'batch_size': batch_size,\n",
    "              'enc_max_len': enc_max_len+1,\n",
    "              'dec_max_len': dec_max_len+2,\n",
    "              'enc_unit': enc_unit,\n",
    "              'dec_unit': dec_unit,\n",
    "              'embed_dim': embed_dim,\n",
    "              'dropout_rate': dropout_rate,\n",
    "              'enc_vocab_size': enc_vocab_size,\n",
    "              'dec_vocab_size': dec_vocab_size,\n",
    "              'dec_sos_token': dec_tokenizer.vocab_size,\n",
    "              'attn_type': attn_type,\n",
    "              'method': method\n",
    "              }\n",
    "    \n",
    "    #model = seq2seq(config)\n",
    "    model = seq2seq_attn(config)\n",
    "    \n",
    "    # checkpoint\n",
    "    checkpoint_dir = 'checkpoint/daily-korean/seq2seq_{}_attn'.format(attn_type)\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, 'checkpoint')\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "    \n",
    "    epoch_loss = tf.keras.metrics.Mean()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss.reset_states()\n",
    "        \n",
    "        train_batches = batch_dataset(dataset, batch_size, enc_tokenizer, dec_tokenizer, enc_max_len, dec_max_len)\n",
    "        #train_batches = batch_iter(dataset, batch_size)\n",
    "        \n",
    "        for batch_idx, (batch_x, batch_y) in enumerate(train_batches):\n",
    "            #batch_x, batch_y = batch_dataset(batch_x, batch_y, word2idx)\n",
    "            loss = 0.\n",
    "            with tf.GradientTape() as tape:\n",
    "                preds, attn_weights = model(batch_x, batch_y, True)\n",
    "\n",
    "                loss = loss_function(batch_y[:, 1:], preds, loss_obj)\n",
    "            \n",
    "            variables = model.trainable_variables\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(gradients, variables))\n",
    "            \n",
    "            epoch_loss(loss)\n",
    "            \n",
    "            if (batch_idx + 1) % log_interval == 0:\n",
    "                print(f'[Epoch {epoch + 1}|Step {batch_idx + 1}/{num_batches_per_epoch}] loss: {loss.numpy()} (Avg. {epoch_loss.result()})')\n",
    "        \n",
    "        model.save_weights(filepath=checkpoint_prefix)\n",
    "        #checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "    print(\"Training is Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_vocab_size: 8633\tdec_vocab_size: 7921\n",
      "11823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0225 03:56:23.038696 140676795627328 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is Done.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(1234)\n",
    "    tf.random.set_seed(1234)\n",
    "    \n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
