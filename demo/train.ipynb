{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JIT Compiled ChatSpace Model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from model.seq2seq.Seq2Seq import *\n",
    "from model.seq2seq_attn.Seq2Seq_Attn import *\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_type = 'luong'\n",
    "method = 'dot'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(true, pred, loss_obj):\n",
    "    mask = tf.math.logical_not(tf.math.equal(true, 0))\n",
    "\n",
    "    loss = loss_obj(true, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    # Load data\n",
    "    dataset = load_dataset(data_dir)\n",
    "    #dataset = load_morp_dataset(dataset)\n",
    "    \n",
    "    num_batches_per_epoch = len(dataset) // batch_size\n",
    "    \n",
    "    # Load tokenizer\n",
    "    enc_tokenizer = load_tokenizer('enc-tokenizer', (x for x, y in dataset), target_vocab_size=2**13)\n",
    "    dec_tokenizer = load_tokenizer('dec-tokenizer', (y for x, y in dataset), target_vocab_size=2**13)\n",
    "    enc_vocab_size = enc_tokenizer.vocab_size + 1\n",
    "    dec_vocab_size = dec_tokenizer.vocab_size + 2\n",
    "    print(f'enc_vocab_size: {enc_vocab_size}\\tdec_vocab_size: {dec_vocab_size}')\n",
    "    \n",
    "    #word2idx = build_dict(dataset)\n",
    "    #idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    #vocab_size = len(word2idx)\n",
    "    #print(\"Size of word2idx is {}\".format(len(word2idx)))\n",
    "    \n",
    "    # Define the optimizer and the loss function\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    \n",
    "    # Define seq2seq model\n",
    "    config = {'batch_size': batch_size,\n",
    "              'enc_max_len': enc_max_len+1,\n",
    "              'dec_max_len': dec_max_len+2,\n",
    "              'enc_unit': enc_unit,\n",
    "              'dec_unit': dec_unit,\n",
    "              'embed_dim': embed_dim,\n",
    "              'dropout_rate': dropout_rate,\n",
    "              'enc_vocab_size': enc_vocab_size,\n",
    "              'dec_vocab_size': dec_vocab_size,\n",
    "              'dec_sos_token': dec_tokenizer.vocab_size,\n",
    "              'attn_type': attn_type,\n",
    "              'method': method\n",
    "              }\n",
    "    \n",
    "    #model = seq2seq(config)\n",
    "    model = seq2seq_attn(config)\n",
    "    \n",
    "    # checkpoint\n",
    "    checkpoint_dir = 'checkpoint/daily-korean/seq2seq_{}_attn'.format(attn_type)\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, 'checkpoint')\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "    \n",
    "    epoch_loss = tf.keras.metrics.Mean()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss.reset_states()\n",
    "        \n",
    "        train_batches = batch_dataset(dataset, batch_size, enc_tokenizer, dec_tokenizer, enc_max_len, dec_max_len)\n",
    "        #train_batches = batch_iter(dataset, batch_size)\n",
    "        \n",
    "        for batch_idx, (batch_x, batch_y) in enumerate(train_batches):\n",
    "            #batch_x, batch_y = batch_dataset(batch_x, batch_y, word2idx)\n",
    "            loss = 0.\n",
    "            with tf.GradientTape() as tape:\n",
    "                preds, attn_weights = model(batch_x, batch_y, True)\n",
    "\n",
    "                loss = loss_function(batch_y[:, 1:], preds, loss_obj)\n",
    "            \n",
    "            variables = model.trainable_variables\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(gradients, variables))\n",
    "            \n",
    "            epoch_loss(loss)\n",
    "            \n",
    "            if (batch_idx + 1) % log_interval == 0:\n",
    "                print(f'[Epoch {epoch + 1}|Step {batch_idx + 1}/{num_batches_per_epoch}] loss: {loss.numpy()} (Avg. {epoch_loss.result()})')\n",
    "        \n",
    "        model.save_weights(filepath=checkpoint_prefix)\n",
    "        #checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "    print(\"Training is Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_vocab_size: 8633\tdec_vocab_size: 7921\n",
      "11823\n",
      "[Epoch 1|Step 50/369] loss: 1.4856469631195068 (Avg. 1.7386714220046997)\n",
      "[Epoch 1|Step 100/369] loss: 1.5124868154525757 (Avg. 1.6123584508895874)\n",
      "[Epoch 1|Step 150/369] loss: 1.4398281574249268 (Avg. 1.5446010828018188)\n",
      "[Epoch 1|Step 200/369] loss: 1.3429992198944092 (Avg. 1.5131295919418335)\n",
      "[Epoch 1|Step 250/369] loss: 1.3298581838607788 (Avg. 1.4850895404815674)\n",
      "[Epoch 1|Step 300/369] loss: 1.2465059757232666 (Avg. 1.4575309753417969)\n",
      "[Epoch 1|Step 350/369] loss: 1.172682762145996 (Avg. 1.4339226484298706)\n",
      "11823\n",
      "[Epoch 2|Step 50/369] loss: 1.149631381034851 (Avg. 1.2216839790344238)\n",
      "[Epoch 2|Step 100/369] loss: 1.2455790042877197 (Avg. 1.2257002592086792)\n",
      "[Epoch 2|Step 150/369] loss: 1.2269008159637451 (Avg. 1.2083889245986938)\n",
      "[Epoch 2|Step 200/369] loss: 1.0734096765518188 (Avg. 1.2010623216629028)\n",
      "[Epoch 2|Step 250/369] loss: 1.1911906003952026 (Avg. 1.1984002590179443)\n",
      "[Epoch 2|Step 300/369] loss: 1.0066502094268799 (Avg. 1.1980574131011963)\n",
      "[Epoch 2|Step 350/369] loss: 1.0588394403457642 (Avg. 1.1948630809783936)\n",
      "11823\n",
      "[Epoch 3|Step 50/369] loss: 1.083641767501831 (Avg. 1.059777855873108)\n",
      "[Epoch 3|Step 100/369] loss: 1.0703380107879639 (Avg. 1.0683698654174805)\n",
      "[Epoch 3|Step 150/369] loss: 1.0511507987976074 (Avg. 1.0566569566726685)\n",
      "[Epoch 3|Step 200/369] loss: 1.1616429090499878 (Avg. 1.0587974786758423)\n",
      "[Epoch 3|Step 250/369] loss: 1.1438454389572144 (Avg. 1.0605396032333374)\n",
      "[Epoch 3|Step 300/369] loss: 1.0389602184295654 (Avg. 1.0584330558776855)\n",
      "[Epoch 3|Step 350/369] loss: 1.1013871431350708 (Avg. 1.0596243143081665)\n",
      "11823\n",
      "[Epoch 4|Step 50/369] loss: 0.9327729344367981 (Avg. 0.8883512616157532)\n",
      "[Epoch 4|Step 100/369] loss: 0.7685126662254333 (Avg. 0.8868820667266846)\n",
      "[Epoch 4|Step 150/369] loss: 0.8190162181854248 (Avg. 0.901791512966156)\n",
      "[Epoch 4|Step 200/369] loss: 0.8299264311790466 (Avg. 0.9081965088844299)\n",
      "[Epoch 4|Step 250/369] loss: 0.9903723001480103 (Avg. 0.9074369072914124)\n",
      "[Epoch 4|Step 300/369] loss: 0.7261167764663696 (Avg. 0.9088120460510254)\n",
      "[Epoch 4|Step 350/369] loss: 0.6908830404281616 (Avg. 0.906294584274292)\n",
      "11823\n",
      "[Epoch 5|Step 50/369] loss: 0.7479594349861145 (Avg. 0.7120622992515564)\n",
      "[Epoch 5|Step 100/369] loss: 0.8651995062828064 (Avg. 0.7290368676185608)\n",
      "[Epoch 5|Step 150/369] loss: 0.6996967792510986 (Avg. 0.7357544898986816)\n",
      "[Epoch 5|Step 200/369] loss: 0.756244421005249 (Avg. 0.7338521480560303)\n",
      "[Epoch 5|Step 250/369] loss: 0.8288628458976746 (Avg. 0.7412907481193542)\n",
      "[Epoch 5|Step 300/369] loss: 0.6905015110969543 (Avg. 0.7462989687919617)\n",
      "[Epoch 5|Step 350/369] loss: 0.7747780680656433 (Avg. 0.747331440448761)\n",
      "11823\n",
      "[Epoch 6|Step 50/369] loss: 0.5221163034439087 (Avg. 0.5706555843353271)\n",
      "[Epoch 6|Step 100/369] loss: 0.7183396220207214 (Avg. 0.5666062831878662)\n",
      "[Epoch 6|Step 150/369] loss: 0.6033536791801453 (Avg. 0.5767568945884705)\n",
      "[Epoch 6|Step 200/369] loss: 0.5147463083267212 (Avg. 0.5803960561752319)\n",
      "[Epoch 6|Step 250/369] loss: 0.7999114990234375 (Avg. 0.5842536091804504)\n",
      "[Epoch 6|Step 300/369] loss: 0.6765042543411255 (Avg. 0.5899093151092529)\n",
      "[Epoch 6|Step 350/369] loss: 0.6265509128570557 (Avg. 0.5948870182037354)\n",
      "11823\n",
      "[Epoch 7|Step 50/369] loss: 0.47544997930526733 (Avg. 0.43297672271728516)\n",
      "[Epoch 7|Step 100/369] loss: 0.5676844120025635 (Avg. 0.43940383195877075)\n",
      "[Epoch 7|Step 150/369] loss: 0.5289772152900696 (Avg. 0.4455544948577881)\n",
      "[Epoch 7|Step 200/369] loss: 0.5243366360664368 (Avg. 0.45148128271102905)\n",
      "[Epoch 7|Step 250/369] loss: 0.5381966233253479 (Avg. 0.4584815204143524)\n",
      "[Epoch 7|Step 300/369] loss: 0.47704648971557617 (Avg. 0.4625367820262909)\n",
      "[Epoch 7|Step 350/369] loss: 0.47954508662223816 (Avg. 0.46663638949394226)\n",
      "11823\n",
      "[Epoch 8|Step 50/369] loss: 0.3510247468948364 (Avg. 0.3284420371055603)\n",
      "[Epoch 8|Step 100/369] loss: 0.34483224153518677 (Avg. 0.3344840109348297)\n",
      "[Epoch 8|Step 150/369] loss: 0.4149477779865265 (Avg. 0.3412204682826996)\n",
      "[Epoch 8|Step 200/369] loss: 0.3763510584831238 (Avg. 0.34653350710868835)\n",
      "[Epoch 8|Step 250/369] loss: 0.3952527344226837 (Avg. 0.34807029366493225)\n",
      "[Epoch 8|Step 300/369] loss: 0.4745948612689972 (Avg. 0.3551761507987976)\n",
      "[Epoch 8|Step 350/369] loss: 0.4668307900428772 (Avg. 0.3600163161754608)\n",
      "11823\n",
      "[Epoch 9|Step 50/369] loss: 0.31141772866249084 (Avg. 0.2649545669555664)\n",
      "[Epoch 9|Step 100/369] loss: 0.2540951073169708 (Avg. 0.25983837246894836)\n",
      "[Epoch 9|Step 150/369] loss: 0.25873735547065735 (Avg. 0.2661634385585785)\n",
      "[Epoch 9|Step 200/369] loss: 0.2687453329563141 (Avg. 0.267142117023468)\n",
      "[Epoch 9|Step 250/369] loss: 0.2952219843864441 (Avg. 0.2692558169364929)\n",
      "[Epoch 9|Step 300/369] loss: 0.33203360438346863 (Avg. 0.27273914217948914)\n",
      "[Epoch 9|Step 350/369] loss: 0.2731378376483917 (Avg. 0.27640941739082336)\n",
      "11823\n",
      "[Epoch 10|Step 50/369] loss: 0.19076849520206451 (Avg. 0.18333040177822113)\n",
      "[Epoch 10|Step 100/369] loss: 0.1909959614276886 (Avg. 0.19085559248924255)\n",
      "[Epoch 10|Step 150/369] loss: 0.21289364993572235 (Avg. 0.19265033304691315)\n",
      "[Epoch 10|Step 200/369] loss: 0.2196456491947174 (Avg. 0.1965731680393219)\n",
      "[Epoch 10|Step 250/369] loss: 0.2466294914484024 (Avg. 0.20041660964488983)\n",
      "[Epoch 10|Step 300/369] loss: 0.16902059316635132 (Avg. 0.20590360462665558)\n",
      "[Epoch 10|Step 350/369] loss: 0.22373555600643158 (Avg. 0.21050171554088593)\n",
      "11823\n",
      "[Epoch 11|Step 50/369] loss: 0.1189621239900589 (Avg. 0.141896054148674)\n",
      "[Epoch 11|Step 100/369] loss: 0.1327298879623413 (Avg. 0.14482718706130981)\n",
      "[Epoch 11|Step 150/369] loss: 0.11188565939664841 (Avg. 0.1522093117237091)\n",
      "[Epoch 11|Step 200/369] loss: 0.19678348302841187 (Avg. 0.15489770472049713)\n",
      "[Epoch 11|Step 250/369] loss: 0.1458357274532318 (Avg. 0.15747492015361786)\n",
      "[Epoch 11|Step 300/369] loss: 0.18521738052368164 (Avg. 0.16006475687026978)\n",
      "[Epoch 11|Step 350/369] loss: 0.1320626586675644 (Avg. 0.16229373216629028)\n",
      "11823\n",
      "[Epoch 12|Step 50/369] loss: 0.10075961798429489 (Avg. 0.1134108453989029)\n",
      "[Epoch 12|Step 100/369] loss: 0.11715733259916306 (Avg. 0.11658389121294022)\n",
      "[Epoch 12|Step 150/369] loss: 0.10528367012739182 (Avg. 0.11603851616382599)\n",
      "[Epoch 12|Step 200/369] loss: 0.13071677088737488 (Avg. 0.11904454976320267)\n",
      "[Epoch 12|Step 250/369] loss: 0.11372603476047516 (Avg. 0.12164966017007828)\n",
      "[Epoch 12|Step 300/369] loss: 0.14140354096889496 (Avg. 0.12426114082336426)\n",
      "[Epoch 12|Step 350/369] loss: 0.11328118294477463 (Avg. 0.1258208006620407)\n",
      "11823\n",
      "[Epoch 13|Step 50/369] loss: 0.1380096822977066 (Avg. 0.08455248177051544)\n",
      "[Epoch 13|Step 100/369] loss: 0.09729553014039993 (Avg. 0.08813843131065369)\n",
      "[Epoch 13|Step 150/369] loss: 0.06107219681143761 (Avg. 0.0885995626449585)\n",
      "[Epoch 13|Step 200/369] loss: 0.17018021643161774 (Avg. 0.09222016483545303)\n",
      "[Epoch 13|Step 250/369] loss: 0.08792328089475632 (Avg. 0.09298506379127502)\n",
      "[Epoch 13|Step 300/369] loss: 0.10458696633577347 (Avg. 0.09524165093898773)\n",
      "[Epoch 13|Step 350/369] loss: 0.07916080951690674 (Avg. 0.09749606251716614)\n",
      "11823\n",
      "[Epoch 14|Step 50/369] loss: 0.07512367516756058 (Avg. 0.07044557482004166)\n",
      "[Epoch 14|Step 100/369] loss: 0.0656709372997284 (Avg. 0.07031258195638657)\n",
      "[Epoch 14|Step 150/369] loss: 0.08919157087802887 (Avg. 0.07156915962696075)\n",
      "[Epoch 14|Step 200/369] loss: 0.062358543276786804 (Avg. 0.0725570023059845)\n",
      "[Epoch 14|Step 250/369] loss: 0.06672855466604233 (Avg. 0.07341734319925308)\n",
      "[Epoch 14|Step 300/369] loss: 0.11608243733644485 (Avg. 0.0750792920589447)\n",
      "[Epoch 14|Step 350/369] loss: 0.120673269033432 (Avg. 0.07628435641527176)\n",
      "11823\n",
      "[Epoch 15|Step 50/369] loss: 0.07124798744916916 (Avg. 0.048558302223682404)\n",
      "[Epoch 15|Step 100/369] loss: 0.060490019619464874 (Avg. 0.05223393067717552)\n",
      "[Epoch 15|Step 150/369] loss: 0.04879867658019066 (Avg. 0.05374768748879433)\n",
      "[Epoch 15|Step 200/369] loss: 0.03397002071142197 (Avg. 0.055003587156534195)\n",
      "[Epoch 15|Step 250/369] loss: 0.062461912631988525 (Avg. 0.05588158220052719)\n",
      "[Epoch 15|Step 300/369] loss: 0.08025316894054413 (Avg. 0.05753462389111519)\n",
      "[Epoch 15|Step 350/369] loss: 0.06128678843379021 (Avg. 0.058983705937862396)\n",
      "11823\n",
      "[Epoch 16|Step 50/369] loss: 0.05898487567901611 (Avg. 0.043309763073921204)\n",
      "[Epoch 16|Step 100/369] loss: 0.045458078384399414 (Avg. 0.04460388049483299)\n",
      "[Epoch 16|Step 150/369] loss: 0.028553243726491928 (Avg. 0.044120904058218)\n",
      "[Epoch 16|Step 200/369] loss: 0.037863753736019135 (Avg. 0.04521551728248596)\n",
      "[Epoch 16|Step 250/369] loss: 0.029273947700858116 (Avg. 0.04627157002687454)\n",
      "[Epoch 16|Step 300/369] loss: 0.06271247565746307 (Avg. 0.047149907797575)\n",
      "[Epoch 16|Step 350/369] loss: 0.07044412195682526 (Avg. 0.048244886100292206)\n",
      "11823\n",
      "[Epoch 17|Step 50/369] loss: 0.035066742449998856 (Avg. 0.03240169584751129)\n",
      "[Epoch 17|Step 100/369] loss: 0.035450324416160583 (Avg. 0.03423619270324707)\n",
      "[Epoch 17|Step 150/369] loss: 0.045766450464725494 (Avg. 0.035748790949583054)\n",
      "[Epoch 17|Step 200/369] loss: 0.04321892559528351 (Avg. 0.0362388975918293)\n",
      "[Epoch 17|Step 250/369] loss: 0.03275829181075096 (Avg. 0.03669188544154167)\n",
      "[Epoch 17|Step 300/369] loss: 0.036977145820856094 (Avg. 0.037288300693035126)\n",
      "[Epoch 17|Step 350/369] loss: 0.05003480613231659 (Avg. 0.03823336586356163)\n",
      "11823\n",
      "[Epoch 18|Step 50/369] loss: 0.02367904968559742 (Avg. 0.026252008974552155)\n",
      "[Epoch 18|Step 100/369] loss: 0.016773540526628494 (Avg. 0.027225010097026825)\n",
      "[Epoch 18|Step 150/369] loss: 0.03147144615650177 (Avg. 0.028667669743299484)\n",
      "[Epoch 18|Step 200/369] loss: 0.042044274508953094 (Avg. 0.028898265212774277)\n",
      "[Epoch 18|Step 250/369] loss: 0.020722446963191032 (Avg. 0.029540346935391426)\n",
      "[Epoch 18|Step 300/369] loss: 0.03373965248465538 (Avg. 0.02994482032954693)\n",
      "[Epoch 18|Step 350/369] loss: 0.0341169610619545 (Avg. 0.030461028218269348)\n",
      "11823\n",
      "[Epoch 19|Step 50/369] loss: 0.040857888758182526 (Avg. 0.020037751644849777)\n",
      "[Epoch 19|Step 100/369] loss: 0.01750839874148369 (Avg. 0.02109627053141594)\n",
      "[Epoch 19|Step 150/369] loss: 0.026882091537117958 (Avg. 0.02160455472767353)\n",
      "[Epoch 19|Step 200/369] loss: 0.020421357825398445 (Avg. 0.022519392892718315)\n",
      "[Epoch 19|Step 250/369] loss: 0.015218847431242466 (Avg. 0.023345643654465675)\n",
      "[Epoch 19|Step 300/369] loss: 0.028541387990117073 (Avg. 0.024164745584130287)\n",
      "[Epoch 19|Step 350/369] loss: 0.04331463202834129 (Avg. 0.024938836693763733)\n",
      "11823\n",
      "[Epoch 20|Step 50/369] loss: 0.028836321085691452 (Avg. 0.01980734057724476)\n",
      "[Epoch 20|Step 100/369] loss: 0.01849481649696827 (Avg. 0.02039000578224659)\n",
      "[Epoch 20|Step 150/369] loss: 0.018099622800946236 (Avg. 0.020580260083079338)\n",
      "[Epoch 20|Step 200/369] loss: 0.022515220567584038 (Avg. 0.02046443521976471)\n",
      "[Epoch 20|Step 250/369] loss: 0.0328819677233696 (Avg. 0.02103407494723797)\n",
      "[Epoch 20|Step 300/369] loss: 0.03677000850439072 (Avg. 0.02157805673778057)\n",
      "[Epoch 20|Step 350/369] loss: 0.012195984832942486 (Avg. 0.022065164521336555)\n",
      "Training is Done.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(1234)\n",
    "    tf.random.set_seed(1234)\n",
    "    \n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
